{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a23113-e616-4de7-81dc-4ad1f12b2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae084a0a-4282-4651-9ab9-d4f114f154fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "folder = 'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d0d4a8-1a97-4015-898d-f82e2cd081ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn categorical data to one hot encoded format\n",
    "def one_hot_encode(df, create_category_if_nan = True):\n",
    "    columns = df.columns.tolist()\n",
    "    categorical = []\n",
    "    for col in df:\n",
    "        if df[col].dtype == 'object':\n",
    "            categorical.append(col)\n",
    "    df = pd.get_dummies(df, columns=categorical, dummy_na=create_category_if_nan)\n",
    "    one_hot_categorical = []\n",
    "    for col in df:\n",
    "        if col not in columns:\n",
    "            one_hot_categorical.append(col)\n",
    "    return df, one_hot_categorical\n",
    "\n",
    "# remove punctuation in column names\n",
    "def clean_column_names(df):\n",
    "    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    return df\n",
    "\n",
    "# drop columns which are completely zeros\n",
    "def clean_empty_columns(df):\n",
    "    drop_col = []\n",
    "    for col in df:\n",
    "        total = df[col].sum()\n",
    "        if total == 0:\n",
    "            drop_col.append(col)\n",
    "    df = df.drop(labels=drop_col,axis=1)\n",
    "    return df\n",
    "\n",
    "# fill in the nan with median of the column for numerical data\n",
    "def fill_na_median(df):\n",
    "    for col in df.columns[df.isna().any()].tolist():\n",
    "        np_array = df[col].to_numpy()\n",
    "        df[col] = df[col].fillna(np.median(np_array[~np.isnan(np_array)]))\n",
    "    return df\n",
    "\n",
    "# check whether lightgbm with gpu is installed\n",
    "def check_gpu_support():\n",
    "    data = np.random.rand(50, 2)\n",
    "    label = np.random.randint(2, size=50)\n",
    "    train_data = lgb.Dataset(data, label=label)\n",
    "    params = {'num_iterations': 1, 'device': 'gpu', 'verbose': -1}\n",
    "    try:\n",
    "        gbm = lgb.train(params, train_set=train_data)\n",
    "        return 'gpu'\n",
    "    except Exception as e:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5e0a87-4d03-4c2f-9978-5b8e6ad343e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load application train and test data\n",
    "train_df = pd.read_csv(os.path.join(folder,'application_train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(folder,'application_test.csv'))\n",
    "# print(\"train_df\",len(train_df),\"test_df\",len(test_df))\n",
    "train_df = train_df.append(test_df)\n",
    "train_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# remove XNA gender because only 4 people is XNA and test set does not have XNA\n",
    "train_df = train_df[train_df['CODE_GENDER'] != 'XNA']\n",
    "# remove extreme outliers in training data\n",
    "train_df = train_df[train_df['AMT_INCOME_TOTAL'] < 20000000]\n",
    "# these values does not make sense from the data description\n",
    "train_df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "train_df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "train_df, train_cat = one_hot_encode(train_df, True)\n",
    "\n",
    "# some feature engineering\n",
    "# a client that is more recently employed may indicate that his income source is not stable\n",
    "train_df['DAYS_EMPLOYED_PERCENT'] = train_df['DAYS_EMPLOYED'] / train_df['DAYS_BIRTH']\n",
    "# a client that has a low income to credit ratio may indicate that his main income is insufficient to deal with emergency situations\n",
    "train_df['INCOME_CREDIT_PERCENT'] = train_df['AMT_INCOME_TOTAL'] / train_df['AMT_CREDIT']\n",
    "# a client with low income per person may indicate a financial hardship\n",
    "train_df['INCOME_PER_PERSON'] = train_df['AMT_INCOME_TOTAL'] / train_df['CNT_FAM_MEMBERS']\n",
    "# a client with high annuity to income ratio maybe more likely to default\n",
    "train_df['ANNUITY_INCOME_PERCENT'] = train_df['AMT_ANNUITY'] / train_df['AMT_INCOME_TOTAL']\n",
    "# a client with high payment rate maybe less likely to default\n",
    "train_df['PAYMENT_RATE'] = train_df['AMT_ANNUITY'] / train_df['AMT_CREDIT']\n",
    "# a client with low goods price to credit ratio maybe more likely to default\n",
    "train_df['GOOD_PRICE_CREDIT_RATIO'] = train_df['AMT_GOODS_PRICE'] / train_df['AMT_CREDIT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "272e2731-f8f7-4a11-9272-1f9fd7dfd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bureau and bureau balance\n",
    "bureau = pd.read_csv(os.path.join(folder,'bureau.csv'))\n",
    "bureau, bureau_categorical = one_hot_encode(bureau,True)\n",
    "\n",
    "bureau_balance = pd.read_csv(os.path.join(folder,'bureau_balance.csv'))\n",
    "bureau_balance, bureau_balance_categorical = one_hot_encode(bureau_balance,True)\n",
    "\n",
    "# aggregation methods for features\n",
    "bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'mean', 'sum', 'size']}\n",
    "for col in bureau_balance_categorical:\n",
    "    bb_aggregations[col] = ['mean']\n",
    "bb_agg = bureau_balance.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "\n",
    "# flatten column names\n",
    "bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "\n",
    "bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
    "\n",
    "# aggregation methods for features\n",
    "# based on model importance\n",
    "bureau_aggregations = {\n",
    "    'DAYS_CREDIT': ['min', 'max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n",
    "    'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "    'AMT_ANNUITY': ['max', 'mean'],\n",
    "    'CNT_CREDIT_PROLONG': ['sum'],\n",
    "    'MONTHS_BALANCE_MIN': ['min'],\n",
    "    'MONTHS_BALANCE_MAX': ['max'],\n",
    "    'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "}\n",
    "\n",
    "# aggregate categorical features\n",
    "for cat in bureau_categorical:\n",
    "    bureau_aggregations[cat] = ['mean']\n",
    "for cat in bureau_balance_categorical:\n",
    "    bureau_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "bureau_aggs = bureau.groupby('SK_ID_CURR').agg(bureau_aggregations)\n",
    "# flatten column names\n",
    "bureau_aggs.columns = pd.Index(['BUREAU_' + e[0] + \"_\" + e[1].upper() for e in bureau_aggs.columns.tolist()])\n",
    "bureau = bureau_aggs\n",
    "\n",
    "train_df = train_df.join(bureau, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4b98b4-9da3-4a72-b642-783601f87b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load credit card balance\n",
    "cc = pd.read_csv(os.path.join(folder,'credit_card_balance.csv'))\n",
    "cc, cat_cols = one_hot_encode(cc,True)\n",
    "\n",
    "cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "\n",
    "# some feature engineering\n",
    "# a client with high amount used from limit maybe more likely to default\n",
    "cc['LIMIT_USE'] = cc['AMT_BALANCE'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "# a client with low current payment to min payment ratio may indicate financial hardship\n",
    "cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] / cc['AMT_INST_MIN_REGULARITY']\n",
    "# whether a client has late payment\n",
    "cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "# a client with high drawing to limit ratio may indicate financial hardship\n",
    "cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "\n",
    "# aggregation methods for features\n",
    "# based on model importance\n",
    "credit_aggregations = {\n",
    "    'MONTHS_BALANCE':['var','min','mean'],\n",
    "    'AMT_BALANCE':['min','max','mean','sum','var'],\n",
    "    'AMT_CREDIT_LIMIT_ACTUAL':['min','max','mean','sum','var'],\n",
    "    'AMT_DRAWINGS_ATM_CURRENT':['max','var','sum','mean'],\n",
    "    'AMT_DRAWINGS_CURRENT':['max','var','sum','mean'],\n",
    "    'AMT_DRAWINGS_OTHER_CURRENT':['mean','var','max'],\n",
    "    'AMT_DRAWINGS_POS_CURRENT':['max','var','sum','mean'],\n",
    "    'AMT_INST_MIN_REGULARITY':['min','max','mean','sum','var'],\n",
    "    'AMT_PAYMENT_CURRENT':['min','max','mean','sum','var'],\n",
    "    'AMT_PAYMENT_TOTAL_CURRENT':['min','max','mean','sum','var'],\n",
    "    'AMT_RECEIVABLE_PRINCIPAL':['min','max','mean','sum','var'],\n",
    "    'AMT_RECIVABLE':['min','max','mean','sum','var'],\n",
    "    'AMT_TOTAL_RECEIVABLE':['min'],\n",
    "    'CNT_DRAWINGS_ATM_CURRENT':['max','mean','sum','var'],\n",
    "    'CNT_DRAWINGS_CURRENT':['max','mean','sum','var'],\n",
    "    'CNT_DRAWINGS_OTHER_CURRENT':['mean','var'],\n",
    "    'CNT_DRAWINGS_POS_CURRENT':['max','mean','sum','var'],\n",
    "    'CNT_INSTALMENT_MATURE_CUM':['max','mean','sum','var'],\n",
    "    'SK_DPD':['min','max','mean','sum','var'],\n",
    "    'SK_DPD_DEF':['max','mean','sum','var'],\n",
    "    'LIMIT_USE': ['max', 'mean'],\n",
    "    'PAYMENT_DIV_MIN': ['min', 'mean'],\n",
    "    'LATE_PAYMENT': ['max', 'sum'],\n",
    "    'DRAWING_LIMIT_RATIO':['min','max','mean','sum','var'],\n",
    "}\n",
    "\n",
    "cc_aggs = cc.groupby('SK_ID_CURR').agg(credit_aggregations)\n",
    "# flatten column names\n",
    "cc_aggs.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_aggs.columns.tolist()])\n",
    "cc = cc_aggs\n",
    "\n",
    "train_df = train_df.join(cc, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8288ac48-92fc-46b5-b02f-51bf9b6b3e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load installments payments\n",
    "ins = pd.read_csv('./input/installments_payments.csv')\n",
    "ins, cat_cols = one_hot_encode(ins,True)\n",
    "\n",
    "# some feature engineering\n",
    "# a client with high payment ratio maybe less likely to default\n",
    "ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "# Days past due and days before due\n",
    "# a client that have past due maybe more likely to default\n",
    "ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "ins['DPD'] = ins['DPD'].apply(lambda x: x if x <= 0 else x)\n",
    "ins['DBD'] = ins['DBD'].apply(lambda x: x if x <= 0 else x)\n",
    "\n",
    "# aggregation methods for features\n",
    "# based on model importance\n",
    "ins_aggregations = {\n",
    "    'SK_ID_PREV': ['size', 'nunique'],\n",
    "    'DPD': ['max', 'mean', 'sum', 'var'],\n",
    "    'DBD': ['max', 'mean', 'sum', 'var'],\n",
    "    'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "    'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "    'AMT_INSTALMENT': ['min','max', 'mean', 'sum'],\n",
    "    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean', 'sum']\n",
    "}\n",
    "\n",
    "# aggregate categorical features\n",
    "for cat in cat_cols:\n",
    "    ins_aggregations[cat] = ['mean']\n",
    "ins_aggs = ins.groupby('SK_ID_CURR').agg(ins_aggregations)\n",
    "# flatten column names\n",
    "ins_aggs.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_aggs.columns.tolist()])\n",
    "ins = ins_aggs\n",
    "\n",
    "train_df = train_df.join(ins, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1758283f-29a9-4570-8707-b6b931dfbc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pos cach balance\n",
    "pos = pd.read_csv('./input/POS_CASH_balance.csv')\n",
    "pos, cat_cols = one_hot_encode(pos,True)\n",
    "\n",
    "# aggregation methods for features\n",
    "# based on model importance\n",
    "pos_aggregations = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'MONTHS_BALANCE': ['min','max', 'mean', 'size'],\n",
    "    'SK_DPD': ['max', 'mean'],\n",
    "    'SK_DPD_DEF': ['max', 'mean']\n",
    "}\n",
    "\n",
    "# aggregate categorical features\n",
    "for cat in cat_cols:\n",
    "    pos_aggregations[cat] = ['mean']\n",
    "pos_aggs = pos.groupby('SK_ID_CURR').agg(pos_aggregations)\n",
    "# flatten column names\n",
    "pos_aggs.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_aggs.columns.tolist()])\n",
    "pos = pos_aggs\n",
    "\n",
    "train_df = train_df.join(pos, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6476423-2795-44e0-bb51-9ddac4b2f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previous applications\n",
    "prev = pd.read_csv('./input/previous_application.csv')\n",
    "prev, cat_cols = one_hot_encode(prev,True)\n",
    "\n",
    "# these values does not make sense from the data description\n",
    "prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "# some feature engineering\n",
    "# amount requested and amount received may indicate the trust of the lender towards the client\n",
    "prev['APP_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "\n",
    "# aggregation methods for features\n",
    "# based on model importance\n",
    "prev_aggregations = {\n",
    "    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "    'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "    'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "    'APP_CREDIT_DIFF':['min', 'max', 'mean', 'var'],\n",
    "    'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "    'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "    'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "    'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'CNT_PAYMENT': ['mean', 'sum', 'max'],\n",
    "    'DAYS_FIRST_DRAWING': ['max', 'mean'],\n",
    "    'DAYS_FIRST_DUE': ['min', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    'DAYS_LAST_DUE': ['max', 'mean'],\n",
    "}\n",
    "\n",
    "# aggregate categorical features\n",
    "for cat in cat_cols:\n",
    "    prev_aggregations[cat] = ['mean']\n",
    "prev_aggs = prev.groupby('SK_ID_CURR').agg(prev_aggregations)\n",
    "# flatten column names\n",
    "prev_aggs.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_aggs.columns.tolist()])\n",
    "prev = prev_aggs\n",
    "\n",
    "train_df = train_df.join(prev, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b48d1c5-3bcf-4e88-90cc-f11c3e711dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final cleaning\n",
    "train_df = clean_column_names(train_df)\n",
    "train_df = clean_empty_columns(train_df)\n",
    "test_df = train_df[train_df['TARGET'].isnull()]\n",
    "train_df = train_df[train_df['TARGET'].notnull()]\n",
    "# print(\"train_df\",len(train_df),\"test_df\",len(test_df))\n",
    "# train_df = fill_na_median(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e0d111-778f-4a40-9ce4-c0a3c3f2f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and testing data\n",
    "train_df_temp = train_df.copy()\n",
    "y = train_df_temp['TARGET']\n",
    "train_feats = [feat for feat in train_df_temp if feat not in ['SK_ID_CURR','TARGET']]\n",
    "train_df_temp = train_df_temp[train_feats]\n",
    "x = train_df_temp\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x, y, test_size=0.1)\n",
    "\n",
    "# prepare submission data\n",
    "test_df_temp = test_df.copy()\n",
    "test_pred = test_df[['SK_ID_CURR','TARGET']]\n",
    "test_pred.reset_index(drop=True,inplace=True)\n",
    "test_feats = [feat for feat in train_df_temp if feat not in ['SK_ID_CURR','TARGET']]\n",
    "test_df_temp = test_df_temp[test_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d71dc7f1-35ae-4aa1-a542-ddff7a1d309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "do_cv = True\n",
    "fast_approximate = False\n",
    "\n",
    "# lgbm settings\n",
    "device = check_gpu_support()\n",
    "lr = 0.005\n",
    "n_leaves = 70\n",
    "n_fold = 10\n",
    "if fast_approximate:\n",
    "    lr = 0.01\n",
    "    n_leaves = 50\n",
    "    n_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd07794-ce38-49b9-a3ea-eed18d5a8a1f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 22392, number of negative: 254358\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75681\n",
      "[LightGBM] [Info] Number of data points in the train set: 276750, number of used features: 635\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1080 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 270 dense feature groups (71.79 MB) transferred to GPU in 0.040276 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Number of positive: 22357, number of negative: 254393\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75681\n",
      "[LightGBM] [Info] Number of data points in the train set: 276750, number of used features: 635\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1080 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 270 dense feature groups (71.79 MB) transferred to GPU in 0.049474 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Number of positive: 22309, number of negative: 254441\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75681\n",
      "[LightGBM] [Info] Number of data points in the train set: 276750, number of used features: 635\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1080 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 270 dense feature groups (71.79 MB) transferred to GPU in 0.042260 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Number of positive: 22264, number of negative: 254486\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75681\n",
      "[LightGBM] [Info] Number of data points in the train set: 276750, number of used features: 635\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1080 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 270 dense feature groups (71.79 MB) transferred to GPU in 0.040276 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Number of positive: 22458, number of negative: 254292\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75681\n",
      "[LightGBM] [Info] Number of data points in the train set: 276750, number of used features: 635\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1080 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 270 dense feature groups (71.79 MB) transferred to GPU in 0.052398 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Number of positive: 22308, number of negative: 254442\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75681\n",
      "[LightGBM] [Info] Number of data points in the train set: 276750, number of used features: 635\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1080 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 270 dense feature groups (71.79 MB) transferred to GPU in 0.047337 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Number of positive: 22316, number of negative: 254434\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75681\n",
      "[LightGBM] [Info] Number of data points in the train set: 276750, number of used features: 635\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1080 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 270 dense feature groups (71.79 MB) transferred to GPU in 0.047421 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Number of positive: 22311, number of negative: 254439\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75681\n",
      "[LightGBM] [Info] Number of data points in the train set: 276750, number of used features: 635\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1080 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 270 dense feature groups (71.79 MB) transferred to GPU in 0.045638 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Number of positive: 22360, number of negative: 254390\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75681\n",
      "[LightGBM] [Info] Number of data points in the train set: 276750, number of used features: 635\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1080 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 270 dense feature groups (71.79 MB) transferred to GPU in 0.051467 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Number of positive: 22341, number of negative: 254409\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75681\n",
      "[LightGBM] [Info] Number of data points in the train set: 276750, number of used features: 635\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1080 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 270 dense feature groups (71.79 MB) transferred to GPU in 0.047251 secs. 1 sparse feature groups\n",
      "[100]\tcv_agg's auc: 0.752074 + 0.00428227\n",
      "[200]\tcv_agg's auc: 0.757406 + 0.00451565\n",
      "[300]\tcv_agg's auc: 0.762494 + 0.00443145\n",
      "[400]\tcv_agg's auc: 0.76708 + 0.0043861\n",
      "[500]\tcv_agg's auc: 0.771368 + 0.00451032\n",
      "[600]\tcv_agg's auc: 0.774893 + 0.00456327\n",
      "[700]\tcv_agg's auc: 0.777777 + 0.00463471\n",
      "[800]\tcv_agg's auc: 0.780065 + 0.00475831\n",
      "[900]\tcv_agg's auc: 0.781863 + 0.00480404\n",
      "[1000]\tcv_agg's auc: 0.783305 + 0.00485444\n",
      "[1100]\tcv_agg's auc: 0.78454 + 0.0049175\n",
      "[1200]\tcv_agg's auc: 0.785534 + 0.00493459\n",
      "[1300]\tcv_agg's auc: 0.786462 + 0.00498214\n",
      "[1400]\tcv_agg's auc: 0.787269 + 0.00498866\n",
      "[1500]\tcv_agg's auc: 0.787906 + 0.00493915\n",
      "[1600]\tcv_agg's auc: 0.788484 + 0.00490893\n",
      "[1700]\tcv_agg's auc: 0.789007 + 0.00491613\n",
      "[1800]\tcv_agg's auc: 0.789476 + 0.00491435\n",
      "[1900]\tcv_agg's auc: 0.789864 + 0.00490729\n",
      "[2000]\tcv_agg's auc: 0.790186 + 0.00495304\n",
      "[2100]\tcv_agg's auc: 0.790487 + 0.00490914\n",
      "[2200]\tcv_agg's auc: 0.790733 + 0.0049331\n",
      "[2300]\tcv_agg's auc: 0.791028 + 0.00492015\n",
      "[2400]\tcv_agg's auc: 0.791266 + 0.0049299\n",
      "[2500]\tcv_agg's auc: 0.79149 + 0.00492152\n",
      "[2600]\tcv_agg's auc: 0.791651 + 0.00489224\n",
      "[2700]\tcv_agg's auc: 0.791801 + 0.00491047\n",
      "[2800]\tcv_agg's auc: 0.791926 + 0.00488974\n",
      "[2900]\tcv_agg's auc: 0.792021 + 0.00490137\n",
      "[3000]\tcv_agg's auc: 0.792143 + 0.00491963\n",
      "[3100]\tcv_agg's auc: 0.792261 + 0.0049259\n",
      "[3200]\tcv_agg's auc: 0.792407 + 0.00489927\n",
      "[3300]\tcv_agg's auc: 0.7925 + 0.00487902\n",
      "[3400]\tcv_agg's auc: 0.792571 + 0.00490239\n",
      "[3500]\tcv_agg's auc: 0.79263 + 0.0048844\n",
      "[3600]\tcv_agg's auc: 0.792721 + 0.00490435\n",
      "[3700]\tcv_agg's auc: 0.792777 + 0.00490567\n",
      "[3800]\tcv_agg's auc: 0.792839 + 0.00488551\n",
      "[3900]\tcv_agg's auc: 0.792901 + 0.00486611\n",
      "[4000]\tcv_agg's auc: 0.792973 + 0.00488797\n",
      "[4100]\tcv_agg's auc: 0.793028 + 0.00490832\n",
      "[4200]\tcv_agg's auc: 0.793078 + 0.00492145\n",
      "[4300]\tcv_agg's auc: 0.793116 + 0.00491047\n",
      "[4400]\tcv_agg's auc: 0.793163 + 0.00493115\n",
      "[4500]\tcv_agg's auc: 0.793183 + 0.0049186\n",
      "[4600]\tcv_agg's auc: 0.793207 + 0.00491072\n",
      "[4700]\tcv_agg's auc: 0.793237 + 0.00493031\n",
      "[4800]\tcv_agg's auc: 0.793266 + 0.00491648\n",
      "[4900]\tcv_agg's auc: 0.793323 + 0.00494437\n",
      "[5000]\tcv_agg's auc: 0.79338 + 0.00495643\n",
      "[5100]\tcv_agg's auc: 0.7934 + 0.00495384\n",
      "[5200]\tcv_agg's auc: 0.793423 + 0.00496283\n",
      "done in 4142s\n",
      "best n_estimators: 5223\n",
      "best cv score: 0.7934371652568645\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "if do_cv:\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt', \n",
    "        'objective': 'binary', \n",
    "        'learning_rate': lr, \n",
    "        'num_leaves': n_leaves, \n",
    "        'max_depth': 10,\n",
    "        'min_data_in_leaf': 15,\n",
    "        'min_child_weight': 0.0001,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'feature_fraction':0.78,\n",
    "        'device_type': device,\n",
    "        }\n",
    "\n",
    "    t0 = time.time()\n",
    "    data_train = lgb.Dataset(x, y, silent=True)\n",
    "    cv_results = lgb.cv(params, data_train, num_boost_round=10000, nfold=n_fold, stratified=False, shuffle=True, metrics='auc',\n",
    "                        early_stopping_rounds=50, verbose_eval=100, show_stdv=True, return_cvbooster=True)\n",
    "    \n",
    "    print(\"done in {:.0f}s\".format(time.time() - t0))\n",
    "\n",
    "    print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "    print('best cv score:', cv_results['auc-mean'][-1])\n",
    "\n",
    "    cvbooster = cv_results['cvbooster']\n",
    "    best_iter = cvbooster.best_iteration\n",
    "    num_boosters = n_fold\n",
    "    preds = np.zeros(test_df.shape[0]) \n",
    "    for i in range(num_boosters):\n",
    "        preds += cvbooster.boosters[i].predict(test_df_temp,num_iteration=best_iter)/num_boosters\n",
    "    test_pred['TARGET'] = preds\n",
    "    test_pred.to_csv('submission_test_cv.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4df0bab7-5e13-4579-9f85-eef6760a657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single model\n",
    "if not do_cv:\n",
    "    t0 = time.time()\n",
    "    # use the hyparameters tuned\n",
    "    model = lgb.LGBMClassifier(objective='binary', boosting_type='gbdt', learning_rate=lr, num_leaves=n_leaves, max_depth=10, n_estimators=5000,\n",
    "                               min_data_in_leaf=15, min_child_weight=0.0001, bagging_fraction=0.8,bagging_freq=5,feature_fraction=0.78,device_type=device, \n",
    "                               stratified=False, shuffle=True, metrics='auc',\n",
    "                               early_stopping_rounds=50, verbose_eval=100, show_stdv=True)\n",
    "    model.fit(X=X_train, y=y_train, eval_set = [(X_train, y_train), (X_valid, y_valid)], early_stopping_rounds=50, verbose=200)\n",
    "    print(\"done in {:.0f}s\".format(time.time() - t0))\n",
    "\n",
    "    preds = model.predict_proba(test_df_temp,num_iteration=model.best_iteration_)[:,1]\n",
    "    test_pred['TARGET'] = preds\n",
    "    test_pred.to_csv('submission_test_single.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5176fc-83c5-4d99-9d64-273653c42bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance from model\n",
    "feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,x.columns)), columns=['Value','Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba9537-e786-4216-b8ad-e43421a4ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize importance\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50,:])\n",
    "plt.title('LightGBM Features')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('lgbm_importance_own_all_features_.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5a315-db0a-4d90-bdad-2647886219d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_imp.to_csv('feature_importance_.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ad3f9-8b5d-4be2-b804-d1cfdb7943a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dgl]",
   "language": "python",
   "name": "conda-env-dgl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
